p8105_hw2_lc3807
================
Linshen Cai
2023-10-04

## problem 0

``` r
library(tidyverse)
library(readxl)
```

## problem 1

``` r
# creat dataframe of month
month_df = 
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )

#import data frame and clean it as follow
pols = 
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") |>
  separate(mon, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |>
  left_join(x = _, y = month_df) |> 
  select(year, month, everything(), -day, -starts_with("prez")) 
```

First, clean the data in pols-month.csv, which shows how many national
politicians are now affiliated with either the Democratic Party or the
Republican Party. There are several instances where `prez_gop` equals
`2`; these are the months in which Ford took office after Nixon
resigned. We code these as `gop` (the same as values when `prez_gop` is
`1`) in the new `president` variable established as part of our data
cleansing.

``` r
snp = 
  read_csv(
    "./data/fivethirtyeight_datasets/snp.csv",
    col_types = cols(date = col_date(format = "%m/%d/%y"))) |>
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, close) 
```

Second, we also clean the 538 `snp` data, which contains information
related to Standard & Poor’s stock market index.

``` r
unemployment = 
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") |>
  rename(year = Year) |>
  pivot_longer(
    Jan:Dec, 
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, unemployment)
```

Finally, we tidy the `unemployment` data so that it can be merged with
the previous datasets.

``` r
data_538 = 
  left_join(pols, snp) |>
  left_join(x = _, y = unemployment)

str(data_538)
## tibble [822 × 13] (S3: tbl_df/tbl/data.frame)
##  $ year        : num [1:822] 1947 1947 1947 1947 1947 ...
##  $ month       : chr [1:822] "January" "February" "March" "April" ...
##  $ month_num   : int [1:822] 1 2 3 4 5 6 7 8 9 10 ...
##  $ gov_gop     : num [1:822] 23 23 23 23 23 23 23 23 23 23 ...
##  $ sen_gop     : num [1:822] 51 51 51 51 51 51 51 51 51 51 ...
##  $ rep_gop     : num [1:822] 253 253 253 253 253 253 253 253 253 253 ...
##  $ gov_dem     : num [1:822] 23 23 23 23 23 23 23 23 23 23 ...
##  $ sen_dem     : num [1:822] 45 45 45 45 45 45 45 45 45 45 ...
##  $ rep_dem     : num [1:822] 198 198 198 198 198 198 198 198 198 198 ...
##  $ president   : chr [1:822] "dem" "dem" "dem" "dem" ...
##  $ month_abb   : chr [1:822] "Jan" "Feb" "Mar" "Apr" ...
##  $ close       : num [1:822] NA NA NA NA NA NA NA NA NA NA ...
##  $ unemployment: num [1:822] NA NA NA NA NA NA NA NA NA NA ...
```

We merge the three datasets!

The ‘pols’ data contains 822 observations and 11 variables, and it
provides information on the distribution of party identification
(Democrat or Republican) among governors and senators for a certain year
from years.The`snp`data consists of 787 observations and 3 variables,
which range from years to months.The `unemployment` data includes 816
observations and 3 variables with a range of years.

## Problem 2

First, I import Mr.Trash Wheel sheet from excel and omit non-data
entries

``` r

#import dataframe use read_excel

mr_trash_wheel_df = 
  read_excel(path = "./data/202309 Trash Wheel Collection Data.xlsx",
             sheet = "Mr. Trash Wheel",
             range = "A2:N586",
             na = c("", "NA", "N/A")
             ) |> 
  janitor::clean_names() |> 
  
  #re-calculate homes powered
  
  mutate(homes_powered = weight_tons * 500 / 30, 
         trash_wheel_type = "Mr. Trash Wheel",
         year = as.double(year)
         )|> 
  select(trash_wheel_type, everything())

mr_trash_wheel_df
## # A tibble: 584 × 15
##    trash_wheel_type dumpster month  year date                weight_tons
##    <chr>               <dbl> <chr> <dbl> <dttm>                    <dbl>
##  1 Mr. Trash Wheel         1 May    2014 2014-05-16 00:00:00        4.31
##  2 Mr. Trash Wheel         2 May    2014 2014-05-16 00:00:00        2.74
##  3 Mr. Trash Wheel         3 May    2014 2014-05-16 00:00:00        3.45
##  4 Mr. Trash Wheel         4 May    2014 2014-05-17 00:00:00        3.1 
##  5 Mr. Trash Wheel         5 May    2014 2014-05-17 00:00:00        4.06
##  6 Mr. Trash Wheel         6 May    2014 2014-05-20 00:00:00        2.71
##  7 Mr. Trash Wheel         7 May    2014 2014-05-21 00:00:00        1.91
##  8 Mr. Trash Wheel         8 May    2014 2014-05-28 00:00:00        3.7 
##  9 Mr. Trash Wheel         9 June   2014 2014-06-05 00:00:00        2.52
## 10 Mr. Trash Wheel        10 June   2014 2014-06-11 00:00:00        3.76
## # ℹ 574 more rows
## # ℹ 9 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
## #   plastic_bags <dbl>, wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>
```

Second, I import Professor Trash Wheel sheet from excel and omit
non-data entries.

``` r

#import dataframe use read_excel
prof_trash_df = 
  read_excel(
    "./data/202309 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    range = "A2:M108",
    na = c("", "NA", "N/A")
    ) |> 
  janitor::clean_names() |> 
  
#re-calculate homes powered
  mutate(
    homes_powered = weight_tons * 500 / 30,
    trash_wheel_type = "Professor Trash Wheel"
    ) |> 
  select(trash_wheel_type, everything())
  
prof_trash_df
## # A tibble: 106 × 14
##    trash_wheel_type      dumpster month     year date                weight_tons
##    <chr>                    <dbl> <chr>    <dbl> <dttm>                    <dbl>
##  1 Professor Trash Wheel        1 January   2017 2017-01-02 00:00:00        1.79
##  2 Professor Trash Wheel        2 January   2017 2017-01-30 00:00:00        1.58
##  3 Professor Trash Wheel        3 February  2017 2017-02-26 00:00:00        2.32
##  4 Professor Trash Wheel        4 February  2017 2017-02-26 00:00:00        3.72
##  5 Professor Trash Wheel        5 February  2017 2017-02-28 00:00:00        1.45
##  6 Professor Trash Wheel        6 March     2017 2017-03-30 00:00:00        1.71
##  7 Professor Trash Wheel        7 April     2017 2017-04-01 00:00:00        1.82
##  8 Professor Trash Wheel        8 April     2017 2017-04-20 00:00:00        2.37
##  9 Professor Trash Wheel        9 May       2017 2017-05-10 00:00:00        2.64
## 10 Professor Trash Wheel       10 May       2017 2017-05-26 00:00:00        2.78
## # ℹ 96 more rows
## # ℹ 8 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
## #   plastic_bags <dbl>, wrappers <dbl>, homes_powered <dbl>
```

Third, I import Gwynnda Trash Wheel sheet from excel and omit non-data
entries.

``` r

#import dataframe use read_excel
gwynnda_trash_df = 
  read_excel(
    "./data/202309 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynnda Trash Wheel",
    range = "A2:L157",
    na = c("", "NA", "N/A")
    ) |> 
  janitor::clean_names() |> 
  
#re-calculate homes powered
  mutate(
    homes_powered = weight_tons * 500 / 30,
    trash_wheel_type = "Gwynnda Trash Wheel"
    ) |> 
  select(trash_wheel_type, everything())

gwynnda_trash_df
## # A tibble: 155 × 13
##    trash_wheel_type    dumpster month   year date                weight_tons
##    <chr>                  <dbl> <chr>  <dbl> <dttm>                    <dbl>
##  1 Gwynnda Trash Wheel        1 July    2021 2021-07-03 00:00:00        0.93
##  2 Gwynnda Trash Wheel        2 July    2021 2021-07-07 00:00:00        2.26
##  3 Gwynnda Trash Wheel        3 July    2021 2021-07-07 00:00:00        1.62
##  4 Gwynnda Trash Wheel        4 July    2021 2021-07-16 00:00:00        1.76
##  5 Gwynnda Trash Wheel        5 July    2021 2021-07-30 00:00:00        1.53
##  6 Gwynnda Trash Wheel        6 August  2021 2021-08-11 00:00:00        2.06
##  7 Gwynnda Trash Wheel        7 August  2021 2021-08-14 00:00:00        1.9 
##  8 Gwynnda Trash Wheel        8 August  2021 2021-08-16 00:00:00        2.16
##  9 Gwynnda Trash Wheel        9 August  2021 2021-08-16 00:00:00        2.6 
## 10 Gwynnda Trash Wheel       10 August  2021 2021-08-17 00:00:00        3.21
## # ℹ 145 more rows
## # ℹ 7 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
## #   polystyrene <dbl>, cigarette_butts <dbl>, plastic_bags <dbl>,
## #   wrappers <dbl>, homes_powered <dbl>
```

Finally, we merge all three dateframes above and calculate total weight
of trash collected by Professor Trash Wheel and the total number of
cigarette butts collected by Gwynnda in July of 2021.

``` r
# merge all three dataframes 
data_all = bind_rows(mr_trash_wheel_df, prof_trash_df, gwynnda_trash_df) 

data_all
## # A tibble: 845 × 15
##    trash_wheel_type dumpster month  year date                weight_tons
##    <chr>               <dbl> <chr> <dbl> <dttm>                    <dbl>
##  1 Mr. Trash Wheel         1 May    2014 2014-05-16 00:00:00        4.31
##  2 Mr. Trash Wheel         2 May    2014 2014-05-16 00:00:00        2.74
##  3 Mr. Trash Wheel         3 May    2014 2014-05-16 00:00:00        3.45
##  4 Mr. Trash Wheel         4 May    2014 2014-05-17 00:00:00        3.1 
##  5 Mr. Trash Wheel         5 May    2014 2014-05-17 00:00:00        4.06
##  6 Mr. Trash Wheel         6 May    2014 2014-05-20 00:00:00        2.71
##  7 Mr. Trash Wheel         7 May    2014 2014-05-21 00:00:00        1.91
##  8 Mr. Trash Wheel         8 May    2014 2014-05-28 00:00:00        3.7 
##  9 Mr. Trash Wheel         9 June   2014 2014-06-05 00:00:00        2.52
## 10 Mr. Trash Wheel        10 June   2014 2014-06-11 00:00:00        3.76
## # ℹ 835 more rows
## # ℹ 9 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
## #   plastic_bags <dbl>, wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>

# calculate total weight of trash collected by Professor Trash Wheel
total_prof_trash_weight = data_all |> 
  filter(trash_wheel_type == "Professor Trash Wheel") |> 
  pull(weight_tons) |> 
  sum()
total_prof_trash_weight
## [1] 216.26

# calculate total number of cigarette butts collected by Gwynnda in July of 2021
total_gwynnda_cigarette_butts_july = data_all |> 
  filter(trash_wheel_type == "Gwynnda Trash Wheel" & month == "July" & year == 2021) |> 
  pull(cigarette_butts) |> 
  sum()
total_gwynnda_cigarette_butts_july
## [1] 16300
```

The `mr_trash_wheel_df` data has 584 observations and 15 variables and
tells us about how many tons of garbage Mr trash wheel collects and what
types of garbage are included(each variables) for a given year from
years 2014 to 2023. The `prof_trash_df` data has 106 observations and 14
variables, ranging from years 2017 to 2023. The `gwynnda_trash_df` data
has 155 observations and 13 variables ranging from years 2021 to 2023.
After we combine three sheets, the `data_all` date fame has845
observations and 15 variables and contains all information about trash
collected from these three different trash wheels for a given year from
2014 to 2023. The total weight of trash collected by Professor Trash
Wheel are 216.26 and the total number of cigarette butts collected by
Gwynnda in July of 2021 are1.63^{4}.

## Problem 3

First,import, clean, and tidy the dataset of baseline demographics and
remove any participants who do not meet the stated inclusion criteria.

``` r
# load the dataset
mci_baseline_df = 
  read_csv("./data/data_mci/MCI_baseline.csv", 
           skip = 1) |>
  
#clean data
  janitor::clean_names() |> 
  
#encode sex and APOE4 carrier status
  mutate(
    sex = ifelse(sex == 1, "male", "female"),
    apoe4 = ifelse(apoe4 == 1, "carrier", "non-carrier" )
         ) 
mci_baseline_df
## # A tibble: 483 × 6
##       id current_age sex    education apoe4       age_at_onset
##    <dbl>       <dbl> <chr>      <dbl> <chr>       <chr>       
##  1     1        63.1 female        16 carrier     .           
##  2     2        65.6 female        20 carrier     .           
##  3     3        62.5 male          16 carrier     66.8        
##  4     4        69.8 female        16 non-carrier .           
##  5     5        66   male          16 non-carrier 68.7        
##  6     6        62.5 male          16 non-carrier .           
##  7     7        66.5 male          18 non-carrier 74          
##  8     8        67.2 female        18 non-carrier .           
##  9     9        66.7 female        16 non-carrier .           
## 10    10        64.1 female        18 non-carrier .           
## # ℹ 473 more rows

# remove the participants who don't meet the stated inclusion criteria
mci_at_baseline_df = mci_baseline_df |>
  filter(current_age < age_at_onset | is.na(age_at_onset))

mci_at_baseline_df
## # A tibble: 93 × 6
##       id current_age sex    education apoe4       age_at_onset
##    <dbl>       <dbl> <chr>      <dbl> <chr>       <chr>       
##  1     3        62.5 male          16 carrier     66.8        
##  2     5        66   male          16 non-carrier 68.7        
##  3     7        66.5 male          18 non-carrier 74          
##  4    13        63.1 male          12 carrier     69          
##  5    14        58.4 female        20 non-carrier 66.2        
##  6    18        67.8 male          16 non-carrier 69.8        
##  7    22        67.3 female        20 carrier     74.6        
##  8    26        64.8 female        20 carrier     71.1        
##  9    30        66.3 female        12 non-carrier 73.1        
## 10    39        68.3 female        16 carrier     70.2        
## # ℹ 83 more rows


# calculate for the average baseline age
base_mean_age_mci = mean(pull(mci_at_baseline_df, current_age))

#calculate the proportion
n_carrier_female = mci_at_baseline_df |> 
  filter(sex == 'female') |> 
  filter(apoe4 == 'carrier') |> 
  count()

n_female = mci_baseline_df |> 
  filter(sex == 'female') |> 
  count()

proportion = n_carrier_female/n_female
```

The first row must be skipped throughout the import procedure because
our dataset does not require remarks.Based on notes, Sex == 0 represents
the participant is female. apoe4 == 0 represents the participant is
non-carrier. Then we use mutate funtion to ensure that sex and APOE4
carrier status are appropriate encoded (i.e. not numeric) according to
notes. We also use filter function to remove any participants who do not
meet the stated inclusion criteria. It means you need to remove any N/A
value in variable `age at onset` variable by using filter function.
There are 483 participants were recruited, 93 develop MCI. The average
baseline age is 65.5419355. The proportion of women in the study are
APOE4 carriers is 0.1421801.

First, Import, clean, and tidy the dataset of longitudinally observed
biomarker values.

``` r
# load the amyloid dataset
mci_amyloid_df = read.csv('./data/data_mci/mci_amyloid.csv', skip = 1) |> 
  janitor::clean_names()

mci_amyloid_df = mci_amyloid_df |> 
  rename(id = study_id,
         second_year = time_2,
         fourth_year = time_4,
         sixth_year = time_6,
         eighth_year = time_8) |> 
  pivot_longer(
    baseline:eighth_year,
    names_to = "time",
    values_to = "value"
    )
```

We still don’t require notes in this dataset, so we skip the first row.
The study_id and time variables are changed to id and years. To make the
dataset easier to understand by clearly displaying time and value, we
use pivot_longer() function. The dateset has some NA values, which
indicates that certain ratio values are missing. There are 2435
observations and 3 variables. The dateframe contains important variables
like id, year, and value once the dataset has been cleaned and
organized.

Second, check whether some participants appear in only the baseline or
amyloid datasets and combine the demographic and biomarker datasets so
that only participants who appear in both datasets are retained.

``` r
#Check participants appearing in only baseline or amyloid datasets
unique_baseline_participants = anti_join(mci_at_baseline_df, mci_amyloid_df,by = "id")

unique_amyloid_participants = anti_join(mci_amyloid_df, mci_at_baseline_df, by = "id")

only_baseline_id = mci_at_baseline_df |> 
  pull(id) |> 
  setdiff(mci_amyloid_df |> pull(id))

only_amyloid_id = mci_amyloid_df |> 
  pull(id) |> 
  setdiff(mci_at_baseline_df |> pull(id))

# combine two datasets together
combined_dataframe = inner_join(mci_at_baseline_df, mci_amyloid_df, by = "id")
```

There are totally 3 participants only appear in baseline dataset :
participants with id 14, 49, 268. There are totally 397 participants
only appear in amyloid dataset : participants with id 1, 2, 4, 6, 8, 9,
10, 11, 12, 15, 16, 17, 19, 20, 21, 23, 24, 25, 27, 28, 29, 31, 32, 33,
34, 35, 36, 37, 38, 40, 41, 42, 44, 46, 47, 48, 50, 51, 52, 53, 54, 56,
57, 58, 60, 61, 62, 63, 64, 66, 68, 70, 71, 72, 73, 74, 79, 80, 81, 82,
83, 84, 85, 88, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,
104, 105, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136,
137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 150, 151, 152,
153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167,
168, 169, 171, 172, 173, 174, 175, 176, 178, 180, 181, 182, 183, 184,
185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,
200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,
215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 227, 228, 231, 232,
233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247,
248, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,
265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 281,
282, 283, 284, 285, 288, 290, 291, 293, 294, 295, 296, 298, 299, 300,
302, 303, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319,
320, 321, 322, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335,
339, 340, 341, 342, 343, 344, 345, 346, 353, 354, 356, 357, 358, 359,
360, 361, 362, 363, 364, 367, 368, 370, 371, 372, 374, 375, 376, 378,
380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394,
395, 396, 397, 399, 401, 402, 403, 404, 405, 406, 407, 410, 411, 413,
414, 415, 418, 420, 421, 422, 425, 427, 428, 429, 430, 432, 433, 434,
435, 436, 437, 438, 439, 441, 443, 444, 445, 446, 447, 450, 451, 454,
455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,
469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,
484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495. There are 90
participants present in both datasets; each participant has 5 rows of
data (baseline to 8th year), hence there are 450 rows total in the
combined dataset. The variables are 8.

``` r
# export the result as a CSV to your data directory
write.csv(combined_dataframe, file = "./data/data_mci/combined_dataset.csv", row.names = T)
```
